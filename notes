ANTS 3 -m PR[fixed.nii.gz,moving.nii.gz,1,2] -i 50x20x10 -o moved.nii.gz -t SyN[0.3] -r Gauss[3,0]
WarpImageMultiTransform 3 moving.nii.gz output.nii.gz -R fixed.nii.gz movedWarp.nii.gz movedAffine.txt 
fslmaths output.nii.gz -thr 300 out.nii.gz




TODO:



Organize files to make them more clear
    We need resampled and segmented datasets, each with their own downstream cube sources for training

Try extracting cubes from the segmented DSB images and store them in a single array, alongside
    a corresponding TSV file that has imgNum | Xind | Yind | Zind | Xpos | Ypos | Zpos information

    This could produce a DSB callback that runs in reasonable time ....


Implement selection between raw and segmented training in FGLab so comparisons can be made

Try evaluating network on LUNA datasets to see if extreme classifications still result

Build softmax representation of noduleSize/cancer classes to see if evaluation improves

Try alternate cube Sizes

Get ANTS working
    See if registration on low-res images can be used to transform high res images
    Try bone-only or lung-only registrations

Use ANTS to transform GRAD maps into a new trainable space



resampledNodulesDF.join(imageDF, on='imgNum')





* query dataframe to see if this box has nodule in it



* for this nodule, find bounding box



old code for making masks ....

def foo():

	# go through all nodes (why just the biggest?)
	for node_idx, cur_row in mini_df.iterrows():
		node_x = cur_row["coordX"]
		node_y = cur_row["coordY"]
		node_z = cur_row["coordZ"]
		diam = cur_row["diameter_mm"]

		# just keep 3 slices
		imgs = np.ndarray([3, height, width], dtype=np.float32)
		masks = np.ndarray([3, height, width], dtype=np.uint8)
		center = np.array([node_x, node_y, node_z])  # nodule center
		v_center = np.rint((center - origin) / spacing)  # nodule center in voxel space (still x,y,z ordering)
		for i, i_z in enumerate(np.arange(int(v_center[2]) - 1,
										  int(v_center[2]) + 2).clip(0, num_z - 1)):  # clip prevents going out of bounds in Z
			mask = make_mask(center, diam, i_z * spacing[2] + origin[2], width, height, spacing, origin)
			masks[i] = mask
			imgs[i] = img_array[i_z]
		np.save(os.path.join(output_path, "images_%04d_%04d.npy" % (fcount, node_idx)), imgs)
		np.save(os.path.join(output_path, "masks_%04d_%04d.npy" % (fcount, node_idx)), masks)



strategy :

loop over images, resample them to reasonable isomorphic dimensions ...
pad resampled image to a consistent dimension, throw into pytables
    hopefully compression will take care of sparse zero regions

generator looping over pytables array:

for each image:
    for each cube:
        is nodule contained in cube?
        (this works, but would need loss weights)


^ could iterate over top to produce two downstream datasets that could be subsampled
    list of nodules
    list of controls


Alternatively, could get around the padding problem and just store smaller cubes
    might need to resample on each run :(
    would need to select cube size ahead of time :(
    we've got the space, so perhaps storing padded full images is best



# stats on nodule sizes
In [71]: len(noduleDF)
Out[71]: 1186

In [72]: noduleDF.diameter_mm.mean()
Out[72]: 8.3065273155539643

In [73]: noduleDF.diameter_mm.std()
Out[73]: 4.7620328071793647





